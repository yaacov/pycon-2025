{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Building a Private RAG Pipeline with a Vector Database\n",
    "\n",
    "Welcome to the final part of our series! So far, we have a private, secure, and optimized LLM running locally. But it's a generalist. It knows about Music Television, but not our internal tools like the **Migration Toolkit for Virtualization (MTV)**.\n",
    "\n",
    "In this notebook, we will solve this by building a Retrieval-Augmented Generation (RAG) pipeline. This will give our LLM a 'memory' of our own private data, without ever needing to be retrained.\n",
    "\n",
    "<img src=\"documentation.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Retrieval-Augmented Generation (RAG)?\n",
    "\n",
    "RAG is a technique for providing an LLM with information from an external knowledge base. Instead of the LLM relying solely on its training data, we first 'retrieve' relevant information and then pass that information to the LLM as context along with the user's query.\n",
    "\n",
    "The process looks like this:\n",
    "1.  **User Query:** The user asks a question (e.g., \"What is MTV?\").\n",
    "2.  **Search/Retrieve:** We search our private knowledge base (a collection of documents) for text chunks that are relevant to the query.\n",
    "3.  **Augment Prompt:** We create a new prompt for the LLM, stuffing it with the relevant text we found. It looks something like: `\"Given this context: [relevant text about Migration Toolkit for Virtualization], answer this question: What is MTV?\"`\n",
    "4.  **Generate:** The LLM, now equipped with the correct context, generates an accurate answer.\n",
    "\n",
    "To do the 'Retrieve' step efficiently, we need a **Vector Database**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the RAG Pipeline\n",
    "\n",
    "We'll use a few key libraries:\n",
    "- **`requests` & `BeautifulSoup4`:** To scrape the documentation for the Migration Toolkit for Virtualization from the web.\n",
    "- **`langchain` & `langchain-community`:** A popular framework that simplifies building applications with LLMs. We'll use it to manage our data, prompts, and the RAG chain itself.\n",
    "- **`sentence-transformers`:** To generate embeddings (numerical representations) of our text.\n",
    "- **`faiss-cpu`:** A library for efficient similarity search. This will be our local vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community langchain-huggingface sentence-transformers faiss-cpu beautifulsoup4 requests optimum[openvino] transformers torch accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Our Private Data\n",
    "\n",
    "First, we need to get the text data for our knowledge base. We'll use two different loaders:\n",
    "1. **HTML Loader**: Scrapes data directly from the Red Hat documentation page\n",
    "2. **Markdown Loader**: Loads data from a local markdown file (`kubectl-mtv.md`)\n",
    "\n",
    "This gives us a comprehensive knowledge base combining both web documentation and local files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data from https://docs.redhat.com/en/documentation/migration_toolkit_for_virtualization/2.0/html/installing_and_using_the_migration_toolkit_for_virtualization/installing-mtv_mtv...\n",
      "HTML data scraped successfully.\n",
      "Loading markdown data from kubectl-mtv.md...\n",
      "Markdown data loaded successfully.\n",
      "Combined documentation loaded: 8403 chars from HTML + 13070 chars from Markdown\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def load_data_from_html_url(url):\n",
    "    \"\"\"HTML Loader: Scrape data from a web URL\"\"\"\n",
    "    print(f\"Scraping data from {url}...\")\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # This selector is specific to the Red Hat documentation page structure\n",
    "    # It might need adjustment for other pages\n",
    "    article_body = soup.find('div', class_='docs-content-container')\n",
    "    if article_body:\n",
    "        print(\"HTML data scraped successfully.\")\n",
    "        return article_body.get_text(separator='\\n', strip=True)\n",
    "    else:\n",
    "        raise ValueError(\"Could not find the main content of the page.\")\n",
    "\n",
    "def load_data_from_markdown_file(file_path):\n",
    "    \"\"\"Markdown Loader: Load data from a local markdown file\"\"\"\n",
    "    print(f\"Loading markdown data from {file_path}...\")\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Markdown file not found: {file_path}\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    print(\"Markdown data loaded successfully.\")\n",
    "    return content\n",
    "\n",
    "# Load data from HTML URL\n",
    "url = \"https://docs.redhat.com/en/documentation/migration_toolkit_for_virtualization/2.0/html/installing_and_using_the_migration_toolkit_for_virtualization/installing-mtv_mtv\"\n",
    "html_docs_text = load_data_from_html_url(url)\n",
    "\n",
    "# Load data from local markdown file\n",
    "markdown_file_path = \"kubectl-mtv.md\"\n",
    "markdown_docs_text = load_data_from_markdown_file(markdown_file_path)\n",
    "\n",
    "# Combine both data sources\n",
    "mtv_docs_text = html_docs_text + \"\\n\\n\" + markdown_docs_text\n",
    "print(f\"Combined documentation loaded: {len(html_docs_text)} chars from HTML + {len(markdown_docs_text)} chars from Markdown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Chunk the Data\n",
    "\n",
    "LLMs have a limited context window. We can't just feed them the entire documentation at once. We need to split the text into smaller, meaningful chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data into 25 chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=100\n",
    ")\n",
    "chunks = text_splitter.split_text(mtv_docs_text)\n",
    "print(f\"Split data into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Embeddings and Store in Vector DB\n",
    "\n",
    "Now for the magic. We'll convert each text chunk into a vector (a list of numbers) using an **embedding model**. These vectors capture the semantic meaning of the text. We'll use a small, efficient embedding model from IBM's Granite family. Then we'll store these vectors in our FAISS vector database.\n",
    "\n",
    "When a user asks a question, we'll convert their question into a vector too and use FAISS to find the text chunks with the most similar vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: ibm-granite/granite-embedding-30m-english\n",
      "Creating vector database...\n",
      "Vector database is ready.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding_model_name = \"ibm-granite/granite-embedding-30m-english\"\n",
    "print(f\"Loading embedding model: {embedding_model_name}\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "print(\"Creating vector database...\")\n",
    "vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})  # Retrieve top 3 most relevant chunks\n",
    "print(\"Vector database is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build the RAG Chain\n",
    "\n",
    "We have all the pieces. Now we'll use LangChain to assemble them into a single, runnable chain. This chain will automatically handle retrieving the context, formatting the prompt, and calling our local, optimized LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/yzamir/.var/app/org.jupyter.JupyterLab/config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:159: OnnxExporterWarning: Symbolic function 'aten::scaled_dot_product_attention' already registered for opset 14. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading optimized LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain is ready.\n"
     ]
    }
   ],
   "source": [
    "from optimum.intel import OVModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set tokenizer parallelism to avoid warnings\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Load our optimized LLM from Part 2\n",
    "llm_model_name = \"ibm-granite/granite-3.3-2b-instruct\"\n",
    "optimized_model_dir = \"./granite-2b-openvino\"\n",
    "print(\"Loading optimized LLM...\")\n",
    "ov_model = OVModelForCausalLM.from_pretrained(optimized_model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "\n",
    "# Create a Hugging Face pipeline with updated parameters\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=ov_model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    top_p=0.95,\n",
    "    return_full_text=False  # Only return the generated text, not the input\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Define the prompt template with better formatting\n",
    "template = \"\"\"You are a helpful assistant with expertise in virtualization and migration technologies. \n",
    "Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer based on the provided context, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# Function to format documents for better context\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Assemble the RAG chain with improved document formatting\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chain is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Ask the Right Question!\n",
    "\n",
    "Now, let's ask our question again. This time, the RAG chain will fetch the relevant context from our vector database and provide it to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying RAG chain with: 'What is MTV?'\n",
      "\n",
      "Response generated in 24.28 seconds.\n",
      "\n",
      "---\n",
      "\n",
      " MTV stands for Migration Toolkit for Virtualization. It is an open-source project that enables the migration of virtual machines (VMs) between different virtualization platforms, such as VMware, Red Hat Virtualization, and OpenShift Virtualization. MTV provides a set of tools and operators to automate and streamline the migration process, making it easier for administrators to move workloads from legacy environments to modern cloud platforms.\n",
      "\n",
      "The MTV Operator is a Kubernetes operator that simplifies the deployment and management of MTV components within an OpenShift cluster. It allows users to install, configure, and manage the Forklift/MTV components, including the ForkliftController, which is responsible for managing the migration process.\n",
      "\n",
      "The provided context describes the steps to install and configure MTV, including the installation of the OpenShift Container Platform web console, OpenShift Virtualization Operator, and MTV Operator. It also outlines the procedure to create a ForkliftController instance and verify the running MTV pods.\n",
      "\n",
      "The context does not explicitly define MTV, but based on the provided steps and terminology, MTV refers to the Migration Toolkit for Virtualization project, which includes the MTV Operator and ForkliftController as key components for VM migrations.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is MTV?\"\n",
    "\n",
    "print(f\"Querying RAG chain with: '{question}'\")\n",
    "start_time = time.time()\n",
    "response = rag_chain.invoke(question)\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f\"\\nResponse generated in {duration:.2f} seconds.\")\n",
    "print(\"\\n---\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: From Generic to Genius\n",
    "\n",
    "Look at the difference! The first time we asked, we got an answer about a music channel. Now, with RAG, our model correctly identifies MTV as the **Migration Toolkit for Virtualization** and provides a detailed, accurate description based on the documentation we provided.\n",
    "\n",
    "We have successfully built a complete, end-to-end private AI pipeline:\n",
    "1.  **Secure:** All data and models remain on our local machine.\n",
    "2.  **Fast:** Optimized with Intel OpenVINO for high-speed inference.\n",
    "3.  **Smart:** Augmented with our own private data using RAG.\n",
    "\n",
    "This architecture is the blueprint for building powerful, secure, and context-aware AI applications for any organization that handles sensitive data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
