{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Accelerating LLM Inference with Intel OpenVINO\n",
    "\n",
    "In the previous notebook, we established a baseline for running an LLM locally. While it worked, the performance might not be fast enough for real-time applications. In this section, we'll use the Intel OpenVINO Toolkit and Optimum-Intel to significantly accelerate our model. This is a crucial step for making local LLMs practical, especially on hardware without high-end GPUs.\n",
    "\n",
    "<img src=\"accelerate.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Intel OpenVINO?\n",
    "\n",
    "**Intel® Distribution of OpenVINO™ Toolkit** is a free toolkit that helps developers optimize and deploy AI inference. It takes a trained model from a framework like PyTorch and optimizes it for high performance on a variety of Intel hardware (CPU, integrated GPU, etc.).\n",
    "\n",
    "One of the key techniques we'll use is **quantization**. This process converts the model's weights from high-precision floating-point numbers (like FP16 or FP32) to lower-precision integers (like INT8). This makes the model smaller and dramatically faster to run, often with a minimal impact on accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Optimize?\n",
    "\n",
    "When you download a model from Hugging Face, it's usually in a generic format. By optimizing it for your specific hardware (in this case, an Intel CPU), you can unlock significant performance gains. This means faster responses, lower latency, and a better user experience, all without changing the model's underlying intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Get Optimizing\n",
    "\n",
    "First, we'll install the necessary libraries. `optimum-intel` is the bridge that connects the Hugging Face ecosystem with Intel's optimization tools like OpenVINO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optimum[openvino] transformers torch accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll load the same `ibm-granite/granite-3.3-2b-instruct` model, but this time we'll use `optimum.intel.OVModelForCausalLM`. We'll instruct it to export the model into an optimized format and apply INT8 quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing model with OpenVINO and INT8 quantization...\n",
      "Loaded previously optimized model.\n",
      "Response generated in 8.07 seconds.\n",
      "\n",
      "---\n",
      "\n",
      "What is MTV?\n",
      "MTV, which stands for Music Television, was an American cable and satellite television network that was founded in 1981. It initially focused on music videos, particularly those by alternative rock artists, but later expanded its programming to include a mix of music-related content such as concerts, interviews, and reality shows. The channel played a significant role in popularizing the music video format during the 1980s and helped launch the careers of many influential bands and artists.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set tokenizer parallelism to avoid warnings\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "model_name = \"ibm-granite/granite-3.3-2b-instruct\"\n",
    "optimized_model_dir = \"./granite-2b-openvino\"\n",
    "\n",
    "# Load the tokenizer as usual\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load and optimize the model for OpenVINO\n",
    "# This will convert the model to OpenVINO format and apply INT8 quantization\n",
    "print(\"Optimizing model with OpenVINO and INT8 quantization...\")\n",
    "\n",
    "# Check if optimized model already exists\n",
    "if Path(optimized_model_dir).exists() and any(Path(optimized_model_dir).iterdir()):\n",
    "    try:\n",
    "        # Try to load a previously optimized model\n",
    "        ov_model = OVModelForCausalLM.from_pretrained(optimized_model_dir)\n",
    "        print(\"Loaded previously optimized model.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load cached model: {e}\")\n",
    "        print(\"Creating new optimized model...\")\n",
    "        ov_model = OVModelForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            export=True, \n",
    "            load_in_8bit=True,\n",
    "            use_cache=True\n",
    "        )\n",
    "        ov_model.save_pretrained(optimized_model_dir)\n",
    "        print(\"Model optimized and saved.\")\n",
    "else:\n",
    "    # If not found, export and quantize it\n",
    "    print(\"Creating new optimized model...\")\n",
    "    ov_model = OVModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        export=True, \n",
    "        load_in_8bit=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "    ov_model.save_pretrained(optimized_model_dir)\n",
    "    print(\"Model optimized and saved.\")\n",
    "\n",
    "def query_optimized_llm(prompt):\n",
    "    # Tokenize with attention mask to avoid warnings\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    \n",
    "    start_time = time.time()\n",
    "    generation_output = ov_model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    response = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"Response generated in {duration:.2f} seconds.\")\n",
    "    print(\"\\n---\\n\")\n",
    "    print(response)\n",
    "\n",
    "query_optimized_llm(\"What is MTV?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison and Conclusion\n",
    "\n",
    "Compare the `Response generated in...` time from this notebook to the time from our first notebook. You should see a **significant reduction**. On a typical modern laptop, the optimized model can be 2-4x faster, or even more!\n",
    "\n",
    "This is the power of hardware-specific optimization. We haven't sacrificed our privacy, but we've made our local LLM much more usable.\n",
    "\n",
    "**The Trade-Off:** The speed increase from quantization can sometimes result in a minor loss of precision. For most tasks, this is unnoticeable. However, for highly sensitive tasks, it's always good to evaluate the quantized model's accuracy against the original.\n",
    "\n",
    "--- \n",
    "\n",
    "Now we have a **private** and **fast** LLM. But it's still generic. In the next and final part, we'll give it specialized knowledge about our organization by setting up a vector database and implementing RAG."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
