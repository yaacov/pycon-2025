{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Baseline Benchmark\n",
    "\n",
    "In this notebook, we'll explore the fundamental reasons why a team might choose to build a private AI pipeline, moving away from cloud-based solutions. We'll discuss the trade-offs and then run a baseline test with a powerful, yet memory-efficient, open-source Large Language Model (LLM) to see how it performs on a laptop.\n",
    "\n",
    "<img src=\"no-cloud.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Great Divide: Cloud vs. Local LLMs\n",
    "\n",
    "**Large Data Centers (The Cloud):**\n",
    "\n",
    "Think of services like Google's NotebookLM, OpenAI's ChatGPT, or Anthropic's Claude. These models are colossal, often with hundreds of billions or even trillions of parameters. They run in massive, distributed data centers with thousands of high-end GPUs.\n",
    "\n",
    "- **Pros:** Incredible power, vast general knowledge, and no need for local hardware investment.\n",
    "- **Cons:** Your data is sent to a third-party server. This is a non-starter for many use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Small Environments (In-House):**\n",
    "\n",
    "This is our focus. A development team, a small company, or even a single researcher needs to work with sensitive data that cannot leave the network. The models are smaller (billions of parameters, not hundreds of billions), and they run on local hardware.\n",
    "\n",
    "- **Pros:** Complete data privacy and security. You have total control over the model and the data.\n",
    "- **Cons:** Requires local hardware (from a powerful laptop to a small server), and smaller models may not have the same breadth of knowledge as their cloud counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases for Offline AI\n",
    "\n",
    "Why go to all this trouble? Here are a few scenarios where private AI is not just a choice, but a necessity:\n",
    "\n",
    "- **Healthcare:** Patient records (PHI) are protected by strict regulations like HIPAA.\n",
    "- **Legal:** Attorney-client privilege and sensitive case data must remain confidential.\n",
    "- **Finance:** Proprietary trading algorithms, financial reports, and customer data are all highly sensitive.\n",
    "- **R&D:** Unpatented research, chemical formulas, and engineering plans are the crown jewels of a company.\n",
    "- **Government/Defense:** Classified information and matters of national security."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hardware Question: Laptop vs. Team Server\n",
    "\n",
    "**Running on a Laptop:**\n",
    "\n",
    "- **Feasibility:** Modern laptops, especially those with a decent amount of RAM and a dedicated GPU (like an NVIDIA RTX series), can run smaller LLMs (1B to 8B parameters) quite effectively. It's perfect for individual development and testing.\n",
    "- **Limitations:** You'll be limited by your laptop's memory and processing power. Running larger models or handling many requests at once will be slow.\n",
    "\n",
    "**Setting up a Team Server:**\n",
    "\n",
    "- **Feasibility:** A dedicated server with one or more powerful GPUs (e.g., NVIDIA A100, H100) can be a shared resource for the entire team. It can run larger, more capable models and serve multiple users simultaneously.\n",
    "- **Investment:** This requires a budget for hardware and someone to maintain it. However, it's often a small price to pay for data security."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark: Vanilla LLM on a Laptop\n",
    "\n",
    "Let's get our hands dirty. We'll install the necessary libraries and download the `ibm-granite/granite-3.3-2b-instruct` model. This is a powerful, open-source model that's small enough to run on a reasonably modern laptop.\n",
    "\n",
    "We'll ask it about \"MTV.\" Without any organization-specific context, it should default to its general knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c7c913185446c8bdbe1ded7c4ef8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response generated in 17.15 seconds.\n",
      "\n",
      "---\n",
      "\n",
      "What is MTV?\n",
      "MTV stands for Music Television, a cable and satellite television network that was founded in 1981. It initially focused on music videos, particularly alternative rock and hip hop, but later expanded its content to include reality TV shows, talk shows, animated programming, live events, and more. The original name of the channel was M2 (Music Television), which changed to MTV in 1987 when it launched nationally in the United States with a new logo design by Spike Jonze and Dave Hill.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "model_name = \"ibm-granite/granite-3.3-2b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"  # Automatically handle device placement\n",
    ")\n",
    "\n",
    "def query_llm(prompt):\n",
    "    # Tokenize with attention mask to avoid warnings\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids = input_ids.to(model.device)\n",
    "        attention_mask = attention_mask.to(model.device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        generation_output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.3,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    response = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"Response generated in {duration:.2f} seconds.\")\n",
    "    print(\"\\n---\\n\")\n",
    "    print(response)\n",
    "\n",
    "query_llm(\"What is MTV?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Outcome\n",
    "\n",
    "The model will almost certainly describe **MTV (Music Television)**, the American cable channel. It will talk about music videos, reality shows like *The Real World*, and its cultural impact. This is the correct \"general knowledge\" answer.\n",
    "\n",
    "However, in our organization, \"MTV\" means **Migration Toolkit for Virtualization**. The model has no way of knowing this. This is the problem that Retrieval-Augmented Generation (RAG) will solve for us in the next notebooks.\n",
    "\n",
    "Next up, we'll see how we can speed up this inference time with some clever optimizations from Intel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
